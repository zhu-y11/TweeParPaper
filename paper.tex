%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn

\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

%%%%%%%
\usepackage{graphicx}
\newcommand{\heart}{\ensuremath\heartsuit}



\title{Parsing Tweets into Universal Dependencies}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

\end{abstract}




\section{Introduction}
Analyzing the syntax of tweets is challenging for traditional NLP tools because most of the tweet texts are informal and noisy.

In this paper, we propose to parse the tweets in the convention of universal dependencies and built up the whole pipeline to parse the tweets from the raw text form.

Contribution of this paper includes:
\begin{itemize}
\item We create a new version of tweet Treebank Tweebank 2.0
\item We propose a neural network method to parse tweets into universal dependencies
\item We study the adaptation of universal dependencies for analyzing tweets
\end{itemize}



\section{Data}
% Linguistic phenomena
	% word level
		% RT
		% Hashtag
		% Mention, urls
		% Symbols and Emoticons
		% Truncated tokens
	% twitter level
		% RT
		% Parataxis	
% POS
	% Acronym priority
	% URL tagged as X	
% Depenency
	% Concentrate on NON-UD phenomena
% Statistics
	% Scale
	% Inter-annotator agreement
	% Proportion
		% how many hashtags, mentions, ....
		% how many acornyms

\subsection{Linguistic Phenomena of Twitter} \label{introduction}
%Eisenstan
As the representative of the web language, different from the standard languages such as newswire, twitter has its own linguistic phenomena, that could be categorized into word level and structure level phenomena, and we will discuss them separately.


\subsubsection{Token Level}

There are special tokens that will appear only or much more frequently in tweets. According to the syntactic functions these tokens have in the tweets, we classify them into three types: syntactic tokens, non-syntactic tokens and optionally syntactic tokens. 
Syntactic tokens are tokens that will always have syntactic functions in tweets, but they might not be in their normal orthographic form and we need to do some preprocessing to recover their standard form. Then we can treat them just as normal tokens in the standard text.
Non-syntactic tokens are tokens that will not have syntactic function or semantic meaning in tweets, and they are not considered in the syntax analysis and need to be treated independently.
Optionally syntactic tokens are tokens that can have either syntactic or non-syntactic functions based on the context. Usually they are tokens like URLs ad hashtags that do not participate in the syntax of the tweets, and we treat them as non-syntactic functions by default. However, similar in the analysis of Kong et al. \shortcite{kong-EtAl:2014:EMNLP2014}, in some cases they are perceived as the arguments or predicates of tweets and therefore have syntactic functions. In this case we treat them as syntactic tokens.

Here is a complete list of the special tokens in tweets classified in three types.

\begin{itemize}
\item syntactic tokens
	\begin{itemize}
		\item phrasal abbreviations: {\tt wtf} (what the fuck), {\tt rn} (right now), etc.
		\item orthographic variants: {\tt u} (you), {\tt sooooo}, {\tt dat} (that), etc.
	\end{itemize}
\item Non-syntactic tokens
	\begin{itemize}
		\item truncated tokens: last tokens that are truncated in tweets exceeding twitter character limits. It might be possible to recover the original tokens from the context in some cases, it is still extremely hard to further predict the rest of the truncated part.
	\end{itemize}
\item Optionally syntactic tokens
	\begin{itemize}
		\item retweet discourse marker: RT
		\item URL: http://bit.ly/xyz
		\item hashtag: \#ACL
		\item at-mentioned username: @user
		\item emoticons and emojis: :), :-), $\heart$, etc.
	\end{itemize}
\end{itemize}

As optionally syntactic tokens will be treated as syntactic or non-syntactic tokens based on the specific context, for the sake of simplicity, we will only discuss the syntactic and non-syntactic tokens in the following sections.

	
\subsubsection{Structure Level}
Besides the token level phenomena of twitter, there are also structural patterns appearing in the twitter.

\begin{itemize}
\item Retweet structure: {\tt RT @user : \{tweet content\}} is a typical structural pattern when a user is retweeting from other users. 
\item At-mentioned structure: Except the retweet structure and syntactically at-mentioned username, we think that all of at-mentioned usernames are representing a vocative situation, usually appearing at the beginning or the end of the sentences in tweets.
\item Paratactic parts without punctuations: We use ``part'' to represent a self-contained clause or phrase, and is independent of the previous and the following part. Very often, one tweet is comprised of multiple parts without any delimiting punctuations, such as {\tt \{part 1\} \{part 2\} ...}
\end{itemize}

We treat retweet structure and at-mention structure as structural patterns and keep their annotations consistent across tweets. For parataxis case, we treat each single sentence or phrases independently, and connect them together following the UD conventions. 

Consider the following example

\begin{figure}[h]
\label{ex1}
\small
{\tt
RT @yijialiu : \#ACL2017 heading to Canada for ACL u gim me money rn :) http://url I would love to do it with th $\ldots$ %…
}
\caption{Tweet Example 1}
\label{ex1}
\end{figure}

From the example, we argue that {\tt u} and {\tt rn} are syntactic tokens.
{\tt th} is a truncated token and does not have any syntactic function, although we can infer that the {\tt th} could be probably {\tt the}.
{\tt \#ACL2017}, {\tt :)} and {\tt http://url} are optionally syntactic tokens, and should be all treated as non-syntactic tokens in this example.
{\tt RT @Yijia :} should be analyzed as the retweet structure, and {\tt heading to Canada for ACL} and {\tt can u gimme some money} should be considered as paratactic clauses without punctuations.

However, consider another example
\begin{figure}[h]
\small
{\tt
@zypandora u gonna $\heart$ it . \#NAACL will be \#Awesome . Check out our new paper with @nlpnoah on http://url . RT it !!
}
\caption{Tweet Example 2}
\label{ex2}
\end{figure}

All of the optionally syntactic tokens ({\tt RT}, {\tt http://url}, {\tt \#NAACL}, {\tt \#Awesome}, {\tt @zypandora}, {\tt @nlpnoah} and {\tt $\heart$}) have syntactic functions, and we need to take them into account in the syntactic analysis.



\subsection{POS Tagging}
We follow the UD V2 morphology guideline and use the Universal POS tags~\cite{PETROV12.274} to tag the tweet tokens. For twitter specific linguistic phenomena, we have set different strategies and try to align with the UD conventions and UD\_English as close as possible.\footnote{https://github.com/UniversalDependencies/UD\_English}

In token level phenomena, for non-syntactic tokens, we tag most of them as X (other), except for the emojis and emoticons, which would be tagged as SYM, as same in the UD\_English. 
For syntactic tokens, we tag them considering their corresponding syntactic function.

In structure level phenomena, we think {\tt RT @user} in retweet structure have no contribution to the tweet syntax and tag both of them as X.
We tag the at-mentioned username in at-mentioned structure as PROPN as they present the vocative case.

Therefore, in Fig. ~\ref{ex1}, {\tt RT} {\tt @yijialiu}, {\tt \#ACL2017}, {\tt http://url} and {\tt th} are all tagged as X and {\tt :)} is tagged as SYM. 
{\tt rn} is tagged as ADV and {\tt ima} is tagged as PROPN.
We should note that multiple tokens are actually included in one non-explicit contraction, and we use one of their tags as the contraction tag according to their dependency relation or semantic priority.
If there is a hierarchy in the contraction, we use the tag of the head word as the contraction tag. ``Right'' is the dependent of ``now '' in the dependency of {\tt rn}, so we tag {\tt rn} as ADV. Although there is no head word in {\tt im}, we think that the subject is usually more informative than the tense in many downstream tasks such as semantic role labelling, so we use the tag of ``I'', e.g. PROPN as the tag of {\tt im}.  
In Fig. ~\ref{ex2}, {\tt RT} and {\tt $\heart$} would be tagged as VERB, {\tt http://url} as X, {\tt \#Awesome} as ADJ, {\tt \#NAACL}, {\tt @zypandora} and {\tt @nlpnoah} as PROPN. One exception is that we tag URLs always as X regardless whether they are syntactic tokens. The reason is that we observed, in UD\_English, that the URLs are all tagged as X and we want to conform with the UD dataset, although we think it is not reasonable and should be changed to tags such as PROPN when they are treated as syntactic tokens.

Comparison with Gimpel
% Tokenization discrepancy



\subsection{Dependency Annotation}
We follow the UD V2 syntax guideline and the English specific Universal Dependency Relations to annotate our data.

In token level phenomena, we adopt the following rules for non-syntactic tokens, denoted as x: 

\begin{itemize}
\item If the sentence x belongs to has a clear predicate with syntactic function, x should not have any dependent and are attached to this sentence’s predicate;
\item If the sentence x belongs to consists of only non-syntactic tokens we attach x to the first tokens of the sentence;
\item In parataxis case, it might be difficult to decide which sentence or phrase x should belong to. In this case, we always attach x to the previous predicate of the paratactic clause or phrase;
\item x is mostly labelled as \it{discourse}, but URLs are always labelled as \it{list};
\end{itemize}

The reason we annotate URLs differently is the same as in POS tagging, that UD\_English labelled them as {\it list} when they are non-syntactic tokens and we want to keep the dataset conformed with it.

Syntactic tokens are just normal tokens.

we further adopt the following rules for structure level phenomena:

\begin{itemize}
\item Retweet structure: we attach {\tt @user} to {\tt RT} and {\tt RT} to the predicate of the following sentence with {\it discourse} as labels.
\item At-mentioned structure: we attach at-mention username to the predicate of the most relevant sentence with {\it vocative} label. If it is hard to decide which sentence would be the most relevant, we attach it to the previous sentence's predicate.
\item Paratactic parts without punctuations: we follow the UD convention, and set the first part as the main part of the sentence, and set its predicate as the main predicate of the sentence. Every predicate of the following parts should attach to the main predicate. If the main part and some following part are both phrases, the label is {\it list}, otherwise the label is {\it parataxis}. Note that we still have an exception for URLs, conforming with UD, which is URLs are always labelled as {\it list}, regardless of the type of the other part.
\end{itemize}




Comparison with Kong


\section{Pipeline}

\section{Model}

\section{Experiments}


\section{Related Work}
Eisenstein \shortcite{eisenstein:2013:NAACL-HLT} reviewed NLP approaches for analyzing text on social media, especially for tweets and showed that there are two major directions for NLP community to handle the tweets, including normalization and domain adaptation. He also pointed out that normalization can be problematic because precisely defining the normalization task is difficult. 

Kong et al. \shortcite{kong-EtAl:2014:EMNLP2014} argues that the Penn Treebank approach to annotation is poorly suited to more informal genres of text, as some of the annotation challenges for tweets,
including token selection, multiword expressions, multiple roots, and structure within noun phrases diverge significantly from conventional approaches. 
They believe that rapid, small scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser, given the rapidly changing nature of tweets~\cite{eisenstein:2013:NAACL-HLT}, the attested difficulties of domain adaptation for parsing~\cite{dred07}, and the expense of creating Penn Treebank-style annotations~\cite{penn93}. 
Therefore, they build a new corpus of tweets (Tweebank), with conventions informed by the domain, using new syntactic annotations that can tackle all the forementioned problems annotated in a day by two dozen annotators, most of whom had only cursory training in the annotation scheme. Then, they modify the decoder of the TurboParser, a graph-based dependency parser, which is open-source and has been found to perform well on a range of parsing problems in different languages~\cite{turbo13} to adapt to the Tweebank dataset, and incorporate new features such as Brown Clusters and Penn Treebank features and changes to specification
in the output space into TurboParser.


Gimpel et al. \shortcite{Gimpel:2011:PTT:2002736.2002747} proposed a set of twitter POS tags that handle most of token level phenomena of tweets. 
We believe that the Universal POS tags have some advantages over their tags on syntax analysis.
The first different abbreviation can have different syntactic functions. 
Like “mfw” is usually followed by an adverbial clause and “ima” is usually followed by a clausal complement. 
It is not reasonable to treat them in the same part-of-speech. 
In this paper, when annotating the POS tagging for abbreviations, we first try to recover their original forms, then use the POS of the core-word as the POS for the abbreviation.
Second, four special POS tags (S, L, M, Y) were designed to handle contraction words in Gimpel et al. \shortcite{Gimpel:2011:PTT:2002736.2002747}. Major concern of designing such tags is to minimize the effort of tokenization. 
However, contractions of common nouns and pronouns are casted into the same category which increase the difficulty of distinguishing their syntactic function (say, there's and book'll are treated with the same syntactic function). What's more, only a small proportion of words can be categorized into these tags (2.7 \% in total), which cast a doubt of the usefulness of these certain tags. In this paper, we believe such contraction can be properly handled by tokenization module, so we suggest to tokenize the contraction word and annotate POS tag accordingly.
Besides the contraction that be conventionally tokenized, tweets also witness a set of unconventional contraction like iv (I've), whatis (what is). In this paper, we follow the same idea of annotation abbreviation to handle the unconventional contractions and use the POS of core word of the original form as their POS.
Third, special POS was designed to handle emoticon in Gimpel et al. \shortcite{Gimpel:2011:PTT:2002736.2002747}. However, in most cases, emoticon plays the same role as most of the symbolic tokens. In this paper, we follow the UD guideline to annotate the emoticon as symbol (SYM).
At last, it’s arguable that some of the hashtags, URLs can work as a nominal in tweets. Whether treating them as the same part-of-speech or different ones according to their context is an open question. A preliminary survey on the standard UD English data shows that URL, email address are all tagged as the foreign language (X), so we also tag them as X and leave the disambiguation of their syntactic function to the annotation of parse tree.


\section{Conclusion}



\bibliography{tacl}
\bibliographystyle{acl2012}
\end{document}



